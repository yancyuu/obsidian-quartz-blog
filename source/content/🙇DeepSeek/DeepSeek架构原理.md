## **Deepseek论文链接：**

https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf

## **快速研读工具：**

https://app.txyz.ai/

## **推荐的公众号文章：**

**家常话讲透DeepSeek**

https://mp.weixin.qq.com/s/uSkGESqx7vNjyWGko1yTNg

**探秘DeepSeek优化“神器”：量化、蒸馏与剪枝！李飞飞团队50美元实现？**

https://mp.weixin.qq.com/s/h82disZVo_XeL2nnXzeWrg

**DeepSeek-R1 是怎么训练的**

https://mp.weixin.qq.com/s/Wuz0H9jmZYV1jM1Y-twTlA

**DeepSeek-R1如何通过知识蒸馏把推理能力迁移到千问Qwen**

https://mp.weixin.qq.com/s/0MbRVsYxiZfY6b_P20bjNA

![](https://lcnkrfbfk2e8.feishu.cn/space/api/box/stream/download/asynccode/?code=NDUzYWY4YmE2OTVlZWJkZWQzYTg3MWYwYmRlYmFkZTRfUTJwNXNUazYyMWl3Y1QxSDMyZXNWN0tVYmhEbDY1N2tfVG9rZW46RWoyUmJibklNbzRvZXR4TXlrTGNRdTBYblJRXzE3NTMwMDk5Njk6MTc1MzAxMzU2OV9WNA)

  

## DeepSeek-V3

DeepSeek-V3是一个拥有671B总参数和37B激活参数的大型混合专家(MoE)语言模型。它采用了**多头潜在注意力(MLA)和DeepSeekMoE**架构,实现了高效推理和经济高效的训练。该模型在14.8万亿个多样化的令牌上进行了预训练,并进行了监督微调和强化学习,在各种评估中都表现出色,仅需2.788M H800 GPU小时就可完成全部训练。

### DeepSeek**创新主要体现在以下几个方面：**

1. **精度重塑**：DeepSeek采用了从32位浮点到8位浮点量化的方法，这种方法在保证模型精度的前提下，大幅降低了训练对显卡内存的需求，节省了高达75%的内存空间，并加快了模型训练过程。
    
2. **纯强化学习方法**：DeepSeek-R1采用了纯强化学习RL的方法，完全摒弃了监督式微调和思维链训练。这种“冷启动”方法类似于AlphaZero在围棋和国际象棋领域的突破，无需借助人类专家的对弈数据，就能从零开始掌握复杂任务。
    
3. **多token系统**：DeepSeek创新性地引入了“多token”系统，使得模型能够一次性读取整个短语甚至句子，提高了处理效率。这项技术使得模型推理速度提升2倍，准确率高达90%，在处理海量文本数据时，效率优势尤为突出。
    

### 架构创新：混合专家模型的"智能路由器"

1. ### **DeepSeekMoE的主要创新点:**
    

- 细粒度的专家分割(fine-grained expert segmentation)，将专家分割成更多更细粒度的专家,以实现更灵活和针对性的知识获取。
    
- 共享专家隔离(shared expert isolation)，专门设置一组专家来捕捉通用知识,减少路由专家之间的冗余。
    
- DeepSeekMoE 2B模型在参数数量相同的情况下,性能优于GShard 2B模型,并且接近其密集型对应模型的性能。
    
- 将DeepSeekMoE扩展到16B参数的规模,其性能与大得多的模型LLaMA2 7B相当,但只使用了40%的计算量。
    
- 进一步扩展到145B参数的规模,DeepSeekMoE一直优于GShard架构。
    

综上所述,DeepSeekMoE的主要创新在于专家分割和隔离机制,使其能够在参数效率和计算效率方面显著优于其他MoE架构。

---

2. ### **MLA创新点**
    

**“多头潜在注意力” = 用更聪明的方式分配算力，让模型在处理长文本时既快又准，还不浪费资源。**

1. #### **和传统注意力有啥不同？**
    

想象一下传统注意力机制（比如 Transformer）的工作原理：

- **传统做法**：模型在处理一句话时，会让每个词都和其他所有词“互相看一遍”（计算所有词对的关系）。
    
- **问题**：如果文本很长（比如一篇论文），计算量爆炸（复杂度是序列长度的平方），显存根本扛不住，速度也慢。
    

**DeepSeek 的改进：**

1. **加一个“摘要层”（潜在空间）**
    
    1. 不是让所有词直接互相看，而是先让模型自动生成几个**关键摘要**（潜在变量），比如“这篇文档在讲深度学习优化方法”。
        
    2. 后续的注意力计算只基于这些“摘要”展开，而不是所有原始词。
        
    3. **效果**：计算量从 O(n²) 降到 O(nk)（k 是摘要数量，远小于 n），显存占用大幅减少。
        
2. **多头分工更智能**
    
    1. 传统多头注意力中，每个“头”可能重复劳动（比如多个头都在看相似的词）。
        
    2. DeepSeek 让不同“头”专注不同类型的摘要：
        
        - 有的头看**局部细节**（比如当前句子）；
            
        - 有的头看**全局主题**（比如整篇文章的关键词）；
            
        - 甚至动态决定每个头该看什么（根据输入内容灵活调整）。
            
    3. **效果**：减少冗余计算，提升信息利用率。
        

3. #### **为什么有效？——工程师关心的实际收益**
    

4. **长文本处理终于不卡了**
    
    1. 传统模型处理长文本（比如法律合同、长代码文件）时容易爆显存，而潜在注意力通过“摘要”压缩信息，让长文本推理速度提升 2-5 倍（假设 k=32，n=1000，计算量从 100万 降到 3.2万）。
        
    2. **应用场景**：文档摘要、代码生成、长对话系统。
        
5. **自动抓住重点，不用手动调**
    
    1. 传统方法可能需要手动设计稀疏注意力（比如只让词看前后 50 个词），但潜在注意力让模型自己学“该关注哪里”。
        
    2. **例子**：在情感分析任务中，模型可能自动生成“情感关键词摘要”（比如“好”、“差”、“推荐”），而不需要人工定义规则。
        
6. **多模态任务更丝滑**
    
    1. 处理图文混合数据时，潜在注意力可以生成跨模态的摘要（比如“图片中的狗”和文本中的“宠物”关联），而不需要暴力计算所有像素和词的关系。
        
    2. **应用场景**：图文生成、视频理解。
        
7. **端侧部署友好**
    
    1. 低计算量 + 低显存占用，适合在手机、IoT 设备上跑大模型（比如手机端的实时翻译）。
        

## DeepSeek- R1

- **架构思路：****在 DeepSeek-V3-Base 模型的基础上，先利用少量高质量的 “冷启动” （Cold Start） 数据进行微调，然后再进行强化学习****。** 这种方法结合了监督学习和强化学习的优势，既可以利用人类的先验知识引导模型，又可以发挥强化学习的自学习和自进化能力。
    
- **冷启动阶段：**使用数千个高质量的人工标注样本对 DeepSeek-V3-Base 模型进行微调，作为强化学习训练的初始模型。为了构建高质量的冷启动数据，DeepSeek 团队尝试了多种方法，包括：
    
    - 使用带有长 CoT 的 few-shot prompting。
        
    - 直接提示模型生成带有反思和验证的详细解答。
        
    - 收集 R1-Zero 的输出，并进行人工标注和格式化。
        
- **面向推理的强化学习：**在冷启动阶段之后，R1 采用了与 R1-Zero 类似的强化学习训练流程，但针对推理任务进行了特别优化。为了解决训练过程中可能出现的语言混杂问题，R1 引入了一个**语言一致性奖励 （Language Consistency Reward）**，该奖励根据 CoT 中目标语言单词的比例来计算。
    
- **拒绝采样与监督微调：**当面向推理的强化学习收敛后，R1 利用训练好的 RL 模型进行**拒绝采样 （Rejection Sampling）**，生成新的 SFT 数据。与之前的冷启动数据不同，这一阶段的 SFT 数据不仅包含推理任务，还涵盖了其他领域的数据，例如写作、角色扮演、问答等，以提升模型的通用能力。
    
- **面向全场景的强化学习：**在收集了新的 SFT 数据后，R1 会进行第二阶段的强化学习训练，这一次，训练的目标不再局限于推理任务，而是涵盖了所有类型的任务。此外， R1 采用了不同的奖励信号和提示分布， 针对不同的任务类型进行了优化。例如， 对于数学、代码和逻辑推理等任务， 采用基于规则的奖励；对于开放式问答、创意写作等任务， 则采用基于模型的奖励。